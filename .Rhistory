starmine.sim$universe <- TRUE
data.interface <- new("sdiDf", data = starmine.sim)
trades.interface <- new("stiFromSignal", in.var = "smi",
+ size = "decile", equity = 1000000, rebal.on = periods$period)
trades.interface <- new("stiFromSignal", in.var="smi", size="decile", equity=1000000, rebal.on=periods$period)
ps <- new("portfolioSim", periods = periods, freq = 12, data.interface = data.interface,
trades.interface = trades.interface, fill.volume.pct = Inf,
out.loc = "out_dir_2", out.type = "lean")
result <- runSim(ps, verbose = FALSE)
summary(result)
summary(result.detail)
summary(result)
summary?
?
out.loc = "out_dir_2", out.type = "lean")
summary(result.detail)
summary(detail.result)
# Requires: cvpreds.R and stack.R
source("https://gist.githubusercontent.com/ledell/f3a87bd136ce06e0a5ff/raw/2a82535892ff66694a1a401de46b8b5a92820849/cvpreds.R")
source("https://gist.githubusercontent.com/ledell/f389ac1e9c6e7000b299/raw/6bc1d2c9cfe1a51ffcdcf79cf184e80a40d4828f/stack.R")
library(h2oEnsemble)  # Requires version >=0.0.4 of h2oEnsemble
library(cvAUC)  # Used to calculate test set AUC (requires version >=1.0.1 of cvAUC)
localH2O <-  h2o.init(nthreads = -1)  # Start an H2O cluster with nthreads = num cores on your machine
# Import a sample binary outcome train/test set into H2O
h2o.init(nthreads = -1)
train_csv <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv"
test_csv <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv"
train <- h2o.importFile(train_csv)
test <- h2o.importFile(test_csv)
y <- "response"
x <- setdiff(names(train), y)
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
family <- "binomial"
nfolds <- 5
glm1 <- h2o.glm(x = x, y = y, family = family,
training_frame = train,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
gbm1 <- h2o.gbm(x = x, y = y, distribution = "bernoulli",
training_frame = train,
nfolds = 5,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
dl1 <- h2o.deeplearning(x = x, y = y, distribution = "bernoulli",
training_frame = train,
nfolds = 5,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
rf1 <- h2o.randomForest(x = x, y = y, #distribution = "bernoulli",
training_frame = train,
seed = 1,
nfolds = 5,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
models <- list(glm1, gbm1, dl1, rf1)
metalearner <- "h2o.glm.wrapper"
stack <- h2o.stack(models,  #models must a list of (supervised) H2O models saved using with keep_levelone_data = TRUE and identical fold_column
response_frame = train[,y],
metalearner = metalearner,
seed = 1,
keep_levelone_data = TRUE)
# Compute test set performance:
perf <- h2o.ensemble_performance(stack, newdata = test)
# The "perf" object has a print method, so we can print results (for the default metric) by simply typing: perf
perf
# H2O Ensemble Performance on <newdata>:
# ----------------
# Family: binomial
# Ensemble performance (AUC): 0.78122076490864
# Base learner performance:
#   learner       AUC
# 1             GLM_model_R_1457489971901_1 0.6823032
# 2            GBM_model_R_1457489971901_19 0.7780807
# 3 DeepLearning_model_R_1457489971901_1124 0.6980040
# 4          DRF_model_R_1457489971901_1153 0.7546005
# For h2oEnsemble v0.1.6 and below, you must generate the performance metrics yourself.
# An example of how to do this is below:
# Generate predictions on the test set
pp <- predict(stack, test)
predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)
labels <- as.data.frame(test[,y])[,1]
# Ensemble test AUC
cvAUC::AUC(predictions = predictions, labels = labels)
# 0.7812278
# Base learner test AUC (for comparison)
L <- length(models)
auc <- sapply(seq(L), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels))
data.frame(learner=names(stack$basefits), auc)
# learner       auc
# 1             GLM_model_R_1457489971901_1 0.6823090
# 2            GBM_model_R_1457489971901_19 0.7780352
# 3 DeepLearning_model_R_1457489971901_1124 0.6979997
# 4          DRF_model_R_1457489971901_1153 0.7545723
install.packages("h2oEnsemble")
install.packages("cvAUC")
install.packages("h2oEnsemble")
library(devtools)
install.packages("devtools")
install_github("h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package")
install.packages("install_github")
install.packages("github")
install.packages("RCurl")
install.packages("RCurl")
install.packages("RCurl")
install.packages("RCurl")
install.packages("h2oEnsemble")
require(devtools)
install_github("h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package")
install.packages("RCurl")
install.packages("RCurl")
install_github("h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package")
require(devtools)
install_github("h2oai/h2o-3/h2o-r/ensemble/h2oEnsemble-package")
source("https://gist.githubusercontent.com/ledell/f3a87bd136ce06e0a5ff/raw/2a82535892ff66694a1a401de46b8b5a92820849/cvpreds.R")
source("https://gist.githubusercontent.com/ledell/f389ac1e9c6e7000b299/raw/6bc1d2c9cfe1a51ffcdcf79cf184e80a40d4828f/stack.R")
library(h2oEnsemble)  # Requires version >=0.0.4 of h2oEnsemble
library(cvAUC)  # Used to calculate test set AUC (requires version >=1.0.1 of cvAUC)
localH2O <-  h2o.init(nthreads = -1)  # Start an H2O cluster with nthreads = num cores on your machine
# Import a sample binary outcome train/test set into H2O
h2o.init(nthreads = -1)
train_csv <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_train_5k.csv"
test_csv <- "https://h2o-public-test-data.s3.amazonaws.com/smalldata/testng/higgs_test_5k.csv"
train <- h2o.importFile(train_csv)
test <- h2o.importFile(test_csv)
y <- "response"
x <- setdiff(names(train), y)
train[,y] <- as.factor(train[,y])
test[,y] <- as.factor(test[,y])
family <- "binomial"
nfolds <- 5
glm1 <- h2o.glm(x = x, y = y, family = family,
training_frame = train,
nfolds = nfolds,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
gbm1 <- h2o.gbm(x = x, y = y, distribution = "bernoulli",
training_frame = train,
nfolds = 5,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
dl1 <- h2o.deeplearning(x = x, y = y, distribution = "bernoulli",
training_frame = train,
nfolds = 5,
fold_assignment = "Modulo",
rf1 <- h2o.randomForest(x = x, y = y, #distribution = "bernoulli",
seed = 1,
training_frame = train,
keep_cross_validation_predictions = TRUE)
nfolds = 5,
fold_assignment = "Modulo",
keep_cross_validation_predictions = TRUE)
models <- list(glm1, gbm1, dl1, rf1)
metalearner <- "h2o.glm.wrapper"
stack <- h2o.stack(models,  #models must a list of (supervised) H2O models saved using with keep_levelone_data = TRUE and identical fold_column
response_frame = train[,y],
metalearner = metalearner,
seed = 1,
keep_levelone_data = TRUE)
# Compute test set performance:
perf <- h2o.ensemble_performance(stack, newdata = test)
# The "perf" object has a print method, so we can print results (for the default metric) by simply typing: perf
perf
# H2O Ensemble Performance on <newdata>:
# ----------------
# Family: binomial
# Ensemble performance (AUC): 0.78122076490864
# Base learner performance:
#   learner       AUC
# 1             GLM_model_R_1457489971901_1 0.6823032
# 2            GBM_model_R_1457489971901_19 0.7780807
# 3 DeepLearning_model_R_1457489971901_1124 0.6980040
# 4          DRF_model_R_1457489971901_1153 0.7546005
# For h2oEnsemble v0.1.6 and below, you must generate the performance metrics yourself.
# An example of how to do this is below:
# Generate predictions on the test set
pp <- predict(stack, test)
predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)
labels <- as.data.frame(test[,y])[,1]
# Ensemble test AUC
cvAUC::AUC(predictions = predictions, labels = labels)
# 0.7812278
# Base learner test AUC (for comparison)
L <- length(models)
auc <- sapply(seq(L), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels))
data.frame(learner=names(stack$basefits), auc)
library(h2oEnsemble)  # Requires version >=0.0.4 of h2oEnsemble
library(cvAUC)  # Used to calculate test set AUC (requires version >=1.0.1 of cvAUC)
metalearner <- "h2o.glm.wrapper"
source('~/H2O/h2o_stacking_example.R')
git clone https://github.com/h2oai/h2o-3.git
packageVersion("h2oEnsemble")
perf <- h2o.ensemble_performance(stack, newdata = test)
perf
pp <- predict(stack, test)
predictions <- as.data.frame(pp$pred)[,3]  #third column, p1 is P(Y==1)
labels <- as.data.frame(test[,y])[,1]
cvAUC::AUC(predictions = predictions, labels = labels)
L <- length(models)
auc <- sapply(seq(L), function(l) cvAUC::AUC(predictions = as.data.frame(pp$basepred)[,l], labels = labels))
data.frame(learner=names(stack$basefits), auc)
source('~/H2O/h2o_deeplearning_gridsearch_mnist_example.R')
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
library(PerformanceAnalytics)
library(PerformanceAnalytics)
update.packages()
# rebalancing analysis for 60/40 US stock/bond portfolio
# load packages
library(quantmod)
library(tseries)
library(PerformanceAnalytics)
# download prices
spy <-get.hist.quote(instrument="spy",start="2003-12-31",quote="AdjClose",compression="d")
agg <-get.hist.quote(instrument="agg",start="2003-12-31",quote="AdjClose",compression="d")
# choose asset weights
w = c(0.6,0.4) # 60% / 40%
# merge price histories into one dataset
# calculate 1-day % returns
# and label columns
portfolio.prices <-as.xts(merge(spy,agg))
portfolio.returns <-na.omit(ROC(portfolio.prices,1,"discrete"))
colnames(portfolio.returns) <-c("spy","agg")
# calculate portfolio total returns
# rebalanced portfolio
portfolio.rebal <-Return.portfolio(portfolio.returns,
rebalance_on="years",
weights=w,wealth.index=TRUE,verbose=TRUE)
# buy and hold portfolio/no rebalancing
portfolio.bh <-Return.portfolio(portfolio.returns,
weights=w,wealth.index=TRUE,verbose=TRUE)
# merge portfolio returns into one dataset
# label columns
portfolios.2 <-cbind(portfolio.rebal$returns,portfolio.bh$returns)
colnames(portfolios.2) <-c("rebalanced","buy and hold")
chart.CumReturns(portfolios.2,
wealth.index=TRUE,
legend.loc="bottomright",
main="Growth of $1 investment",
ylab="$")
source('~/QA/Project_MUTB/PerformanceAnalytics.R')
source('~/QA/Project_MUTB/PerformanceAnalytics.R')
ylab="$")
install.packages("rCrl")
install.packages("rClr")
install.packages("rClr")
install.packages("PerformanceAnalytics")
install.packages("Portfolioanalytics")
install.packages("PortfolioAnalytics")
install.packages("factorAnalytics")
install.packages("DEoptim")
library(dplyr)
library(PerformanceAnalytics)
library(PortfolioAnalytics)
data(managers)
dat <- managers
charts.PerformanceSummary(dat, colorset=rainbow(ncol(dat)),
main="Performance Summary", methods=c("StdDev", "HistoricalVaR"))
spec <- portfolio.spec(assets=names(dat)) %>%
add.constraint(type="weight_sum", min_sum=-2, max_sum=2) %>%
add.constraint(type="box", min=-0.2, max=0.2) %>%
add.constraint(type="position_limit", max_pos=ncol(dat)) %>%
add.objective(type="return", name="mean") %>%
add.objective(type="risk", name="StdDev")
rp <- random_portfolios(spec, 5000, "sample")
port.opt <- optimize.portfolio(dat, spec, optimize_method="random", rp=rp, trace=TRUE)
plot(port.opt, main="Optimal Portfolio", risk.col="StdDev", neighbors=10)
spec <- portfolio.spec(assets=names(dat)) %>%
add.constraint(type="weight_sum", min_sum=-2, max_sum=2) %>%
add.constraint(type="box", min=-0.2, max=0.2) %>%
add.constraint(type="position_limit", max_pos=ncol(dat)) %>%
add.objective(type="return", name="mean") %>%
add.objective(type="risk", name="StdDev")
rp <- random_portfolios(spec, 5000, "sample")
port.opt <- optimize.portfolio(dat, spec, optimize_method="random", rp=rp, trace=TRUE)
plot(port.opt, main="Optimal Portfolio", risk.col="StdDev", neighbors=10)
bt <- suppressMessages(optimize.portfolio.rebalancing(
dat, spec, optimize_method="DEoptim", rebalance_on="quarters", training_period=70, traceDE=0))
chart.Weights(bt, main="weights plot", col=rainbow(ncol(dat)))
spec <- portfolio.spec(assets=names(dat)) %>%
add.objective(type="risk", name="StdDev")
library()
library(dplyr)
library(PerformanceAnalytics)
library(PortfolioAnalytics)
data(managers)
dat <- managers
charts.PerformanceSummary(dat, colorset=rainbow(ncol(dat)),
main="Performance Summary", methods=c("StdDev", "HistoricalVaR"))
spec <- portfolio.spec(assets=names(dat)) %>%
add.constraint(type="weight_sum", min_sum=-2, max_sum=2) %>%
add.constraint(type="box", min=-0.2, max=0.2) %>%
add.constraint(type="position_limit", max_pos=ncol(dat)) %>%
add.objective(type="return", name="mean") %>%
add.objective(type="risk", name="StdDev")
rp <- random_portfolios(spec, 5000, "sample")
library(DEoptim)
spec <- portfolio.spec(assets=names(dat)) %>%
add.objective(type="risk", name="StdDev")
bt <- suppressMessages(optimize.portfolio.rebalancing(
dat, spec, optimize_method="DEoptim", rebalance_on="quarters", training_period=70, traceDE=0))
chart.Weights(bt, main="weights plot", col=rainbow(ncol(dat)))
demo(DEoptim())
demo(DEoptim())
demo()
opt.meucci <- optimize.portfolio(R, portfolio=meanSD.portf,
optimize_method="random",
rp=rp,
trace=TRUE,
momentargs=m.moments)
source(file = )
dir
dir()
library(PortfolioAnalytics)
source("data_prep.R")
R <- edhec[,1:5]
# mix of blue, green, and red hues
my_colors <- c("#a6cee3", "#1f78b4", "#b2df8a", "#33a02c", "#fb9a99", "#e31a1c")
##### RP Demo #####
portf.lo <- portfolio.spec(colnames(R))
portf.lo <- add.constraint(portf.lo, type="weight_sum",
min_sum=0.99, max_sum=1.01)
portf.lo <- add.constraint(portf.lo, type="long_only")
# Generate random portfolios using the 3 methods
rp1 <- random_portfolios(portf.lo, permutations=2000, rp_method='sample')
rp2 <- random_portfolios(portf.lo, permutations=2000, rp_method='simplex')
rp3 <- random_portfolios(portf.lo, permutations=2000, rp_method='grid')
# Calculate the portfolio mean return and standard deviation
rp1_mean <- apply(rp1, 1, function(x) mean(R %*% x))
rp1_StdDev <- apply(rp1, 1, function(x) StdDev(R, weights=x))
rp2_mean <- apply(rp2, 1, function(x) mean(R %*% x))
rp2_StdDev <- apply(rp2, 1, function(x) StdDev(R, weights=x))
rp3_mean <- apply(rp3, 1, function(x) mean(R %*% x))
rp3_StdDev <- apply(rp3, 1, function(x) StdDev(R, weights=x))
x.assets <- StdDev(R)
y.assets <- colMeans(R)
###
require(rCharts)
# create an interactive plot using rCharts and nvd3 scatterChart
tmp1 <- data.frame(name="sample", mean=rp1_mean, sd=rp1_StdDev)
tmp2 <- data.frame(name="simplex", mean=rp2_mean, sd=rp2_StdDev)
tmp3 <- data.frame(name="grid", mean=rp3_mean, sd=rp3_StdDev)
tmp <- rbind(tmp1, tmp2, tmp3)
rp_viz <- nPlot(mean ~ sd, group="name", data=tmp, type="scatterChart")
rp_viz$xAxis(
axisLabel = 'Risk (std. dev.)'
,tickFormat = "#!d3.format('0.4f')!#"
)
rp_viz$yAxis(
axisLabel = 'Return'
,tickFormat = "#!d3.format('0.4f')!#"
)
rp_viz$chart(color = my_colors[c(2,4,6)])
#set left margin so y axis label will show up
rp_viz$chart( margin = list(left = 100) )
# rp_viz$chart(
# tooltipContent = "#!
# function(a,b,c,d) {
# //d has all the info you need
# return( '<h3>' + d.point.series + '</h3>Return: ' + d.point.y + '<br>Risk: ' + d.point.x)
# }
# !#")
####if you do not want fisheye/magnify
####let me know, and will show how to remove
####this will solve the tooltip problem
save(rp_viz, file="figures/rp_viz.rda")
###
x.lower <- min(x.assets) * 0.9
x.upper <- max(x.assets) * 1.1
y.lower <- min(y.assets) * 0.9
y.upper <- max(y.assets) * 1.1
png("figures/rp_plot.png", height = 500, width = 1000)
# plot feasible portfolios
plot(x=rp1_StdDev, y=rp1_mean, col=my_colors[2], main="Random Portfolio Methods",
ylab="mean", xlab="StdDev", xlim=c(x.lower, x.upper),
ylim=c(y.lower, y.upper))
points(x=rp2_StdDev, y=rp2_mean, col=my_colors[4], pch=2)
points(x=rp3_StdDev, y=rp3_mean, col=my_colors[6], pch=5)
points(x=x.assets, y=y.assets)
text(x=x.assets, y=y.assets, labels=colnames(R), pos=4, cex=0.8)
legend("bottomright", legend=c("sample", "simplex", "grid"),
col=my_colors[c(2,4,6)],
pch=c(1, 2, 5), bty="n")
dev.off()
png("figures/fev_plot.png", height = 500, width = 1000)
fev <- 0:5
x.assets <- StdDev(R)
y.assets <- colMeans(R)
par(mfrow=c(2, 3))
for(i in 1:length(fev)){
rp <- rp_simplex(portfolio=portf.lo, permutations=2000, fev=fev[i])
tmp.mean <- apply(rp, 1, function(x) mean(R %*% x))
tmp.StdDev <- apply(rp, 1, function(x) StdDev(R=R, weights=x))
x.lower <- min(c(tmp.StdDev, x.assets)) * 0.85
x.upper <- max(c(tmp.StdDev, x.assets)) * 1.15
y.lower <- min(c(tmp.mean, y.assets)) * 0.85
y.upper <- max(c(tmp.mean, y.assets)) * 1.15
plot(x=tmp.StdDev, y=tmp.mean, main=paste("FEV =", fev[i]),
ylab="mean", xlab="StdDev", col=rgb(0, 0, 100, 50, maxColorValue=255),
xlim=c(x.lower, x.upper),
ylim=c(y.lower, y.upper))
points(x=x.assets, y=y.assets)
text(x=x.assets, y=y.assets, labels=colnames(R), pos=4, cex=0.8)
}
par(mfrow=c(1,1))
dev.off()
library(PortfolioAnalytics)
source("data_prep.R")
R <- edhec[,1:5]
dir()
source(file="data_prep.r")
source(file="./data_prep.r")
source("./data_prep.r")
source("C:\Users\u6040067\Documents")
source("C:\Users\u6040067\Documents\data_prep.r")
source("C:\Users\u6040067\Documents\data_prep.r")
library(PortfolioAnalytics)
library(quantmod)
library(PerformanceAnalytics)
library(zoo)
library(plotly)
install.packages("plotly")
install.packages("shiny")
install.packages("PerformanceAnalytics")
install.packages("markdown")
install.packages("PerformanceAnalytics")
install.packages("PerformanceAnalytics")
install.packages("PerformanceAnalytics")
library(XML)
install.packages("XML")
library(XML)
install.packages("PortfolioAnalytics")
update.packages()
require(RODBC)
ch <- odbcConnect("DSN=qad;UID=Ryoichi.Naito;PWD=Ryoichi")
library(arules)
data(Groceries)
Groceries
summary(Groceries)
# è¡ŒåEã«å¤‰æ›ã€å“ç›®åã‚’åˆ—åã¨ã—ãŸè¡ŒåEãŒä½œæEã•ã‚Œã‚Emtx <- as(Groceries, "matrix")
head(mtx[,1:7])
# ã‚¢ã‚¤ãƒEƒ ã®å‡ºç¾é »åº¦
itemFrequencyPlot(Groceries, horiz=T)
# ã‚¢ã‚½ã‚·ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãƒ«ãƒ¼ãƒ«å®Ÿè¡ŒãEãŸã‚apriorié–¢æ•°ã‚’ä½¿ç”¨
# ãƒEƒ•ã‚©ãƒ«ãƒˆã§ã¾ãšãEã‚¢ãƒ—ãƒªã‚ªãƒªå®Ÿè¡Egroceries.ap <- apriori(Groceries)
groceries.ap <- apriori(Groceries, parameter=list(support=0.001,
confidence=0.001,
minlen=3,
maxlen=3)))
groceries.ap <- apriori(Groceries, parameter=list(support=0.001,
confidence=0.001,
minlen=3,
maxlen=3))
inspect(head(sort(groceries.ap, by="lift"), 3))
inspect(head(sort(groceries.ap, by="lift"), 10))
install.packages("mxnet")
install.packages("xmnet")
update.packages("h2o")
library(h2o)
library(h2o)
install.packages("jsonlite")
library(h2o)
a <- 100
b <- 20
x <- matrix(rnorm(a * b), ncol=a, nrow=b)
write.table("./makeResult.txt")
cir
dir()
a <- 100
b <- 20
x <- matrix(rnorm(a * b), ncol=a, nrow=b)
write.table('makeResult.txt')
a <- 100
b <- 20
x <- matrix(rnorm(a * b), ncol=a, nrow=b)
x
install.packages("RMeCab", repos="http://rmecab.jp/R")
library(RMeCab)
res <- RMeCabC("‚·‚à‚à‚à‚à‚à‚à‚à‚à‚Ì‚¤‚¿")
res
q()
rm(list=ls())
#install.packages("plotrix")
library(plotrix)
#install.packages("RMeCab", repos="http://rmecab.jp/R")
library(RMeCab)
# å˜èªžæ„Ÿæƒ…æ¥µæ€§å¯¾å¿œè¡¨ã®å–å¾—
pndic <- read.table("http://www.lr.pi.titech.ac.jp/~takamura/pubs/pn_ja.dic",
sep=":",
col.names = c("term","kana","pos","value"),
colClasses=c("character","character","factor","numeric"),
fileEncoding = "SJIS")
head(pndic)
# åè©žï¼‹å“è©žã§è¤‡æ•°ã®å€™è£œãŒã‚ã‚‹å ´åˆã¯å¹³å‡å€¤ã‚’æŽ¡ç”¨
pndic2 <- aggregate(value ~ term + pos,pndic,mean)
# pndic2$value <- pndic2$value + 0.35
pndic2$value <- pndic2$value + 0.50
getwd()
setwd("GitHub/NaitoNewsAnalytics")
oil <- RMeCabFreq("oil_20161201_1.txt")
# pndicã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å˜èªž(term)ã®ã¿æŠ½å‡º
oil <- subset(oil,Term %in% pndic2$term)
# å˜èªžæ„Ÿæƒ…æ¥µæ€§ã‚¹ã‚³ã‚¢ä»˜ä¸Ž
oil <- merge(oil,pndic2,by.x=c("Term","Info1"),by.y=c("term","pos"))
oil
# #å˜èªžã®å‡ºç¾å›žæ•°ã«ã‚¹ã‚³ã‚¢ã‚’ã‹ã‘ã¦ç·å’Œã‚’ã¨ã‚‹
scoreOil <- oil[4:(ncol(oil)-1)] * oil$value
oil <-c(sum(scoreOil > 0),sum(scoreOil < 0))
lbls <- c("positive", "negative")
lbls <- paste(lbls, oil, sep="ï¼š")
pie3D(oil, radius=0.9, labels=lbls, explode=0.1, main="Naito Analyticsã«ã‚ˆã‚‹ãƒã‚¸ãƒã‚¬(åŽŸæ²¹)ãƒ½('A`)ï¾‰ ")
